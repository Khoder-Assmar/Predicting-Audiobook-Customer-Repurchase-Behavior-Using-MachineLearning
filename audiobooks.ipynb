{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796.0 3579 0.5018161497625034\n",
      "210.0 447 0.4697986577181208\n",
      "231.0 448 0.515625\n",
      "Epoch 1/100\n",
      "36/36 - 2s - 43ms/step - accuracy: 0.6502 - loss: 0.6034 - val_accuracy: 0.7405 - val_loss: 0.5031\n",
      "Epoch 2/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.7564 - loss: 0.4689 - val_accuracy: 0.7808 - val_loss: 0.4213\n",
      "Epoch 3/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.7795 - loss: 0.4202 - val_accuracy: 0.7875 - val_loss: 0.3888\n",
      "Epoch 4/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.7899 - loss: 0.3914 - val_accuracy: 0.7919 - val_loss: 0.3670\n",
      "Epoch 5/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.7946 - loss: 0.3773 - val_accuracy: 0.8166 - val_loss: 0.3498\n",
      "Epoch 6/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8039 - loss: 0.3666 - val_accuracy: 0.7964 - val_loss: 0.3453\n",
      "Epoch 7/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8111 - loss: 0.3573 - val_accuracy: 0.8054 - val_loss: 0.3423\n",
      "Epoch 8/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8192 - loss: 0.3519 - val_accuracy: 0.8322 - val_loss: 0.3306\n",
      "Epoch 9/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8150 - loss: 0.3454 - val_accuracy: 0.8143 - val_loss: 0.3301\n",
      "Epoch 10/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8201 - loss: 0.3436 - val_accuracy: 0.7942 - val_loss: 0.3330\n",
      "Epoch 11/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8229 - loss: 0.3403 - val_accuracy: 0.8031 - val_loss: 0.3271\n",
      "Epoch 12/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8206 - loss: 0.3360 - val_accuracy: 0.8210 - val_loss: 0.3209\n",
      "Epoch 13/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8229 - loss: 0.3343 - val_accuracy: 0.8345 - val_loss: 0.3190\n",
      "Epoch 14/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8209 - loss: 0.3317 - val_accuracy: 0.8076 - val_loss: 0.3237\n",
      "Epoch 15/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8215 - loss: 0.3329 - val_accuracy: 0.7942 - val_loss: 0.3315\n",
      "Epoch 16/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8237 - loss: 0.3317 - val_accuracy: 0.8098 - val_loss: 0.3193\n",
      "Epoch 17/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8217 - loss: 0.3314 - val_accuracy: 0.8054 - val_loss: 0.3206\n",
      "Epoch 18/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8195 - loss: 0.3310 - val_accuracy: 0.8143 - val_loss: 0.3220\n",
      "Epoch 19/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8237 - loss: 0.3287 - val_accuracy: 0.8345 - val_loss: 0.3117\n",
      "Epoch 20/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8187 - loss: 0.3307 - val_accuracy: 0.8121 - val_loss: 0.3201\n",
      "Epoch 21/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8178 - loss: 0.3309 - val_accuracy: 0.7987 - val_loss: 0.3171\n",
      "Epoch 22/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8223 - loss: 0.3242 - val_accuracy: 0.8233 - val_loss: 0.3098\n",
      "Epoch 23/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8312 - loss: 0.3210 - val_accuracy: 0.8322 - val_loss: 0.3078\n",
      "Epoch 24/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.8315 - loss: 0.3191 - val_accuracy: 0.8233 - val_loss: 0.3141\n",
      "Epoch 25/100\n",
      "36/36 - 0s - 6ms/step - accuracy: 0.8284 - loss: 0.3199 - val_accuracy: 0.8210 - val_loss: 0.3130\n",
      "Epoch 26/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8245 - loss: 0.3216 - val_accuracy: 0.8233 - val_loss: 0.3076\n",
      "Epoch 27/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8279 - loss: 0.3185 - val_accuracy: 0.8166 - val_loss: 0.3097\n",
      "Epoch 28/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8262 - loss: 0.3183 - val_accuracy: 0.8389 - val_loss: 0.3036\n",
      "Epoch 29/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8304 - loss: 0.3164 - val_accuracy: 0.8009 - val_loss: 0.3236\n",
      "Epoch 30/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8293 - loss: 0.3175 - val_accuracy: 0.8188 - val_loss: 0.3086\n",
      "Epoch 31/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8198 - loss: 0.3198 - val_accuracy: 0.8166 - val_loss: 0.3101\n",
      "Epoch 32/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8254 - loss: 0.3186 - val_accuracy: 0.8031 - val_loss: 0.3113\n",
      "Epoch 33/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8276 - loss: 0.3174 - val_accuracy: 0.8277 - val_loss: 0.3066\n",
      "Epoch 34/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8282 - loss: 0.3183 - val_accuracy: 0.8255 - val_loss: 0.3027\n",
      "Epoch 35/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8251 - loss: 0.3155 - val_accuracy: 0.8098 - val_loss: 0.3164\n",
      "Epoch 36/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.8265 - loss: 0.3154 - val_accuracy: 0.7964 - val_loss: 0.3168\n",
      "Epoch 37/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8245 - loss: 0.3186 - val_accuracy: 0.8345 - val_loss: 0.3025\n",
      "Epoch 38/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8326 - loss: 0.3141 - val_accuracy: 0.8322 - val_loss: 0.3020\n",
      "Epoch 39/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8321 - loss: 0.3137 - val_accuracy: 0.8210 - val_loss: 0.3043\n",
      "Epoch 40/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8268 - loss: 0.3131 - val_accuracy: 0.8188 - val_loss: 0.3072\n",
      "Epoch 41/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8312 - loss: 0.3116 - val_accuracy: 0.8523 - val_loss: 0.2986\n",
      "Epoch 42/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8268 - loss: 0.3137 - val_accuracy: 0.8121 - val_loss: 0.3038\n",
      "Epoch 43/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8365 - loss: 0.3122 - val_accuracy: 0.8345 - val_loss: 0.3016\n",
      "Epoch 44/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8332 - loss: 0.3120 - val_accuracy: 0.8345 - val_loss: 0.3006\n",
      "Epoch 45/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8321 - loss: 0.3107 - val_accuracy: 0.8233 - val_loss: 0.3031\n",
      "Epoch 46/100\n",
      "36/36 - 0s - 6ms/step - accuracy: 0.8318 - loss: 0.3119 - val_accuracy: 0.8277 - val_loss: 0.2992\n",
      "Epoch 47/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8265 - loss: 0.3104 - val_accuracy: 0.8098 - val_loss: 0.3046\n",
      "Epoch 48/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8332 - loss: 0.3093 - val_accuracy: 0.8166 - val_loss: 0.3019\n",
      "Epoch 49/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8304 - loss: 0.3118 - val_accuracy: 0.8434 - val_loss: 0.3007\n",
      "Epoch 50/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.8360 - loss: 0.3109 - val_accuracy: 0.8412 - val_loss: 0.2975\n",
      "Epoch 51/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8298 - loss: 0.3090 - val_accuracy: 0.8277 - val_loss: 0.3011\n",
      "Epoch 52/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8287 - loss: 0.3122 - val_accuracy: 0.8210 - val_loss: 0.2981\n",
      "Epoch 53/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8254 - loss: 0.3101 - val_accuracy: 0.8031 - val_loss: 0.3034\n",
      "Epoch 54/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8338 - loss: 0.3085 - val_accuracy: 0.8210 - val_loss: 0.3050\n",
      "Epoch 55/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8332 - loss: 0.3087 - val_accuracy: 0.8076 - val_loss: 0.3046\n",
      "Epoch 56/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8298 - loss: 0.3101 - val_accuracy: 0.8277 - val_loss: 0.3028\n",
      "Epoch 57/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8296 - loss: 0.3091 - val_accuracy: 0.8456 - val_loss: 0.2991\n",
      "Epoch 58/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8310 - loss: 0.3110 - val_accuracy: 0.8188 - val_loss: 0.3125\n",
      "Epoch 59/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8349 - loss: 0.3094 - val_accuracy: 0.8389 - val_loss: 0.2989\n",
      "Epoch 60/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8265 - loss: 0.3097 - val_accuracy: 0.8367 - val_loss: 0.2967\n",
      "Epoch 61/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8363 - loss: 0.3068 - val_accuracy: 0.8188 - val_loss: 0.2976\n",
      "Epoch 62/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8363 - loss: 0.3063 - val_accuracy: 0.8277 - val_loss: 0.2982\n",
      "Epoch 63/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8293 - loss: 0.3110 - val_accuracy: 0.8210 - val_loss: 0.2988\n",
      "Epoch 64/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8296 - loss: 0.3097 - val_accuracy: 0.8389 - val_loss: 0.2977\n",
      "Epoch 65/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8329 - loss: 0.3082 - val_accuracy: 0.8098 - val_loss: 0.3051\n",
      "Epoch 66/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8340 - loss: 0.3075 - val_accuracy: 0.8188 - val_loss: 0.3113\n",
      "Epoch 67/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8273 - loss: 0.3161 - val_accuracy: 0.8143 - val_loss: 0.3051\n",
      "Epoch 68/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.8324 - loss: 0.3084 - val_accuracy: 0.8166 - val_loss: 0.3052\n",
      "Epoch 69/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8270 - loss: 0.3080 - val_accuracy: 0.8255 - val_loss: 0.3023\n",
      "Epoch 70/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8349 - loss: 0.3092 - val_accuracy: 0.8345 - val_loss: 0.2977\n",
      "Epoch 71/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8343 - loss: 0.3065 - val_accuracy: 0.8255 - val_loss: 0.2984\n",
      "Epoch 72/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8310 - loss: 0.3061 - val_accuracy: 0.8233 - val_loss: 0.2991\n",
      "Epoch 73/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8310 - loss: 0.3068 - val_accuracy: 0.8367 - val_loss: 0.3029\n",
      "Epoch 74/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8388 - loss: 0.3043 - val_accuracy: 0.8412 - val_loss: 0.2961\n",
      "Epoch 75/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.8310 - loss: 0.3074 - val_accuracy: 0.8233 - val_loss: 0.2985\n",
      "Epoch 76/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8326 - loss: 0.3080 - val_accuracy: 0.8166 - val_loss: 0.3063\n",
      "Epoch 77/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8268 - loss: 0.3117 - val_accuracy: 0.8188 - val_loss: 0.2981\n",
      "Epoch 78/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.8318 - loss: 0.3038 - val_accuracy: 0.8143 - val_loss: 0.2979\n",
      "Epoch 79/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8354 - loss: 0.3047 - val_accuracy: 0.8233 - val_loss: 0.2984\n",
      "Epoch 80/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8282 - loss: 0.3081 - val_accuracy: 0.8054 - val_loss: 0.3081\n",
      "Epoch 81/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8296 - loss: 0.3101 - val_accuracy: 0.8255 - val_loss: 0.3004\n",
      "Epoch 82/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8259 - loss: 0.3078 - val_accuracy: 0.8322 - val_loss: 0.2963\n",
      "Epoch 83/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8329 - loss: 0.3073 - val_accuracy: 0.8188 - val_loss: 0.3049\n",
      "Epoch 84/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8343 - loss: 0.3083 - val_accuracy: 0.8479 - val_loss: 0.2936\n",
      "Epoch 85/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8363 - loss: 0.3034 - val_accuracy: 0.8210 - val_loss: 0.2951\n",
      "Epoch 86/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8340 - loss: 0.3052 - val_accuracy: 0.8277 - val_loss: 0.2949\n",
      "Epoch 87/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8343 - loss: 0.3049 - val_accuracy: 0.8277 - val_loss: 0.2962\n",
      "Epoch 88/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8340 - loss: 0.3022 - val_accuracy: 0.8143 - val_loss: 0.2995\n",
      "Epoch 89/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8346 - loss: 0.3068 - val_accuracy: 0.8300 - val_loss: 0.3042\n",
      "Epoch 90/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8354 - loss: 0.3046 - val_accuracy: 0.8188 - val_loss: 0.3062\n",
      "Epoch 91/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8310 - loss: 0.3091 - val_accuracy: 0.8076 - val_loss: 0.2980\n",
      "Epoch 92/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8340 - loss: 0.3080 - val_accuracy: 0.8210 - val_loss: 0.3136\n",
      "Epoch 93/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8363 - loss: 0.3052 - val_accuracy: 0.8345 - val_loss: 0.2949\n",
      "Epoch 94/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8312 - loss: 0.3065 - val_accuracy: 0.8434 - val_loss: 0.2963\n",
      "Epoch 95/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8321 - loss: 0.3052 - val_accuracy: 0.8277 - val_loss: 0.2962\n",
      "Epoch 96/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8363 - loss: 0.3037 - val_accuracy: 0.8255 - val_loss: 0.2991\n",
      "Epoch 97/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8310 - loss: 0.3046 - val_accuracy: 0.8233 - val_loss: 0.3017\n",
      "Epoch 98/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8329 - loss: 0.3056 - val_accuracy: 0.8166 - val_loss: 0.3024\n",
      "Epoch 99/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8357 - loss: 0.3043 - val_accuracy: 0.8121 - val_loss: 0.3035\n",
      "Epoch 100/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8332 - loss: 0.3025 - val_accuracy: 0.8188 - val_loss: 0.2976\n",
      "Epoch 1/100\n",
      "36/36 - 1s - 33ms/step - accuracy: 0.6873 - loss: 0.5869 - val_accuracy: 0.7248 - val_loss: 0.5095\n",
      "Epoch 2/100\n",
      "36/36 - 0s - 6ms/step - accuracy: 0.7639 - loss: 0.4698 - val_accuracy: 0.7673 - val_loss: 0.4266\n",
      "Epoch 3/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.7835 - loss: 0.4176 - val_accuracy: 0.8076 - val_loss: 0.3795\n",
      "Epoch 4/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.7949 - loss: 0.3908 - val_accuracy: 0.8210 - val_loss: 0.3603\n",
      "Epoch 5/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.7991 - loss: 0.3787 - val_accuracy: 0.8009 - val_loss: 0.3506\n",
      "Epoch 6/100\n",
      "36/36 - 0s - 6ms/step - accuracy: 0.8013 - loss: 0.3645 - val_accuracy: 0.8233 - val_loss: 0.3378\n",
      "Epoch 7/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.8111 - loss: 0.3593 - val_accuracy: 0.7852 - val_loss: 0.3403\n",
      "Epoch 8/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8078 - loss: 0.3541 - val_accuracy: 0.8031 - val_loss: 0.3278\n",
      "Epoch 9/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8120 - loss: 0.3459 - val_accuracy: 0.8121 - val_loss: 0.3250\n",
      "Epoch 10/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8131 - loss: 0.3434 - val_accuracy: 0.8479 - val_loss: 0.3196\n",
      "Epoch 11/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8170 - loss: 0.3394 - val_accuracy: 0.8277 - val_loss: 0.3157\n",
      "Epoch 12/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8212 - loss: 0.3366 - val_accuracy: 0.8367 - val_loss: 0.3143\n",
      "Epoch 13/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.8164 - loss: 0.3384 - val_accuracy: 0.8300 - val_loss: 0.3099\n",
      "Epoch 14/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8209 - loss: 0.3334 - val_accuracy: 0.8121 - val_loss: 0.3223\n",
      "Epoch 15/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8175 - loss: 0.3340 - val_accuracy: 0.8166 - val_loss: 0.3106\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8057 - loss: 0.3591 \n",
      "Test loss: 0.36. Test accuracy: 78.79%\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Precision: 0.80\n",
      "Recall: 0.78\n",
      "F1 Score: 0.79\n",
      "Confusion Matrix:\n",
      " [[172  45]\n",
      " [ 50 181]]\n",
      "AUC-ROC: 0.90\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Audiobooks business case\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Preprocess the data. Balance the dataset. Create 3 datasets: training, validation, and test. Save the newly created sets in a tensor friendly format (e.g. *.npz)\n",
    "# \n",
    "# Since we are dealing with real life data, we will need to preprocess it a bit. This is the relevant code, which is not that hard, but is crucial to creating a good model.\n",
    "# \n",
    "# If you want to know how to do that, go through the code. In any case, this should do the trick for most datasets organized in the way: many inputs, and then 1 cell containing the targets (supervised learning datasets). Keep in mind that a specific problem may require additional preprocessing.\n",
    "# \n",
    "# Note that we have removed the header row, which contains the names of the categories. We simply want the data.\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Extract the data from the csv\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Load the CSV data into a NumPy array.\n",
    "# 'Audiobooks_data.csv' is the file name and ',' is the delimiter used in the CSV file.\n",
    "raw_csv_data = np.loadtxt('001 Audiobooks-data.csv', delimiter=',')\n",
    "\n",
    "# Extract the input features from the raw data.\n",
    "# All columns except the first (index 0) and the last (index -1) are considered input features.\n",
    "unscaled_inputs_all = raw_csv_data[:, 1:-1]\n",
    "\n",
    "# Extract the target values (labels) from the raw data.\n",
    "# The last column (index -1) is considered the target.\n",
    "targets_all = raw_csv_data[:, -1]\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Balance the dataset\n",
    "\n",
    "# %%\n",
    "# 1. We will count the number of targets that are 1\n",
    "# 2. We will keep as many 0's as 1's (We will delete the others)\n",
    "\n",
    "# Count the number of target values that are 1.\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "\n",
    "# Initialize a counter for the number of 0 targets encountered.\n",
    "zero_targets_counter = 0\n",
    "\n",
    "# Initialize a list to store the indices of rows to be removed.\n",
    "indices_to_remove = []\n",
    "\n",
    "# Iterate over all the target values.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    # The shape of targets_all on axis=0 is basically the length of the column.\n",
    "    if targets_all[i] == 0:\n",
    "        # Increment the counter for 0 targets.\n",
    "        zero_targets_counter += 1\n",
    "        # If the count of 0 targets exceeds the number of 1 targets,\n",
    "        # append the current index to the list of indices to remove.\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "# Remove the rows corresponding to the indices in 'indices_to_remove' from 'unscaled_inputs_all'.\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "\n",
    "# Remove the rows corresponding to the indices in 'indices_to_remove' from 'targets_all'.\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Standardize the inputs\n",
    "\n",
    "# %%\n",
    "# Scale the input features to have a mean of 0 and a standard deviation of 1.\n",
    "# This is done to normalize the data, which helps in faster convergence during training.\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Shuffle the data\n",
    "\n",
    "# %%\n",
    "# Create an array of indices corresponding to the number of samples.\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "# Shuffle the indices randomly to ensure the data is mixed well.\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Reorder the input features using the shuffled indices.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "# Reorder the target values using the shuffled indices.\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Split the data into training, validation, and test \n",
    "\n",
    "# %%\n",
    "# STEP 1:\n",
    "\n",
    "# Calculate the total number of samples in the dataset.\n",
    "samples_count = shuffled_indices.shape[0]\n",
    "\n",
    "# STEP 2:\n",
    "\n",
    "# Calculate the number of training samples (80% of the total samples).\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "\n",
    "# Calculate the number of validation samples (10% of the total samples).\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "# Calculate the number of test samples.\n",
    "# The test samples count is the remaining samples after allocating for training and validation.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# STEP 3:\n",
    "\n",
    "# Select the training data.\n",
    "# The training inputs are the first 'train_samples_count' samples from 'shuffled_inputs'.\n",
    "train_input = shuffled_inputs[:train_samples_count]\n",
    "\n",
    "# The training targets are the first 'train_samples_count' samples from 'shuffled_targets'.\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Select the validation data.\n",
    "# The validation inputs are the next 'validation_samples_count' samples after the training samples.\n",
    "validation_input = shuffled_inputs[train_samples_count:train_samples_count + validation_samples_count]\n",
    "\n",
    "# The validation targets are the next 'validation_samples_count' samples after the training samples.\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count + validation_samples_count]\n",
    "\n",
    "# Select the test data.\n",
    "# The test inputs are the remaining samples after the training and validation samples.\n",
    "test_input = shuffled_inputs[train_samples_count + validation_samples_count:]\n",
    "\n",
    "# The test targets are the remaining samples after the training and validation samples.\n",
    "test_targets = shuffled_targets[train_samples_count + validation_samples_count:]\n",
    "\n",
    "# Print the sum of targets and counts for train, validation, and test sets.\n",
    "# This helps to check the balance of the dataset after the split.\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)\n",
    "\n",
    "# This process ensures that the data is divided into training, validation, and test sets with the proportions of 80%, 10%, and 10% respectively, \n",
    "# which helps in training the model, validating it, and testing its performance.\n",
    "\n",
    "\n",
    "# %%\n",
    "# Training Data: 1798.0 3579 0.502374965074043\n",
    "\n",
    "# 1798.0: The sum of the target values for the training set. Since the target values are binary (0 or 1), this means there are 1798 samples with the target value of 1.\n",
    "# 3579: The total number of samples in the training set.\n",
    "# 0.502374965074043: The proportion of samples with the target value of 1 in the training set. \n",
    "# This is calculated as 1798 / 3579, which is approximately 50.24%. \n",
    "# This indicates that the training set is roughly balanced with respect to the target classes (1s and 0s).\n",
    "\n",
    "# Validation Data: 210.0 447 0.4697986577181208\n",
    "\n",
    "# 210.0: The sum of the target values for the validation set. There are 210 samples with the target value of 1.\n",
    "# 447: The total number of samples in the validation set.\n",
    "# 0.4697986577181208: The proportion of samples with the target value of 1 in the validation set. \n",
    "# This is calculated as 210 / 447, which is approximately 46.98%. \n",
    "# This shows that the validation set is slightly imbalanced but still reasonably close to 50%.\n",
    "\n",
    "# Test Data: 229.0 448 0.5111607142857143\n",
    "\n",
    "# 229.0: The sum of the target values for the test set. There are 229 samples with the target value of 1.\n",
    "# 448: The total number of samples in the test set.\n",
    "# 0.5111607142857143: The proportion of samples with the target value of 1 in the test set. \n",
    "# This is calculated as 229 / 448, which is approximately 51.12%. \n",
    "# This indicates that the test set is also roughly balanced.\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Save the three datasets in *.npz\n",
    "\n",
    "# %%\n",
    "# Save the three datasets in *.npz:\n",
    "np.savez('Audiobooks_data_train.npz', inputs=train_input, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation.npz', inputs=validation_input, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test.npz', inputs=test_input, targets=test_targets)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Problem\n",
    "# \n",
    "# You are given data from an Audiobook app. Logically, it relates only to the audio versions of books. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\n",
    "# \n",
    "# The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertizing to him/her. If we can focus our efforts ONLY on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities.\n",
    "# \n",
    "# You have a .csv summarizing the data. There are several variables: Customer ID, Book length in mins_avg (average of all purchases), Book length in minutes_sum (sum of all purchases), Price Paid_avg (average of all purchases), Price paid_sum (sum of all purchases), Review (a Boolean variable), Review (out of 10), Total minutes listened, Completion (from 0 to 1), Support requests (number), and Last visited minus purchase date (in days).\n",
    "# \n",
    "# So these are the inputs (excluding customer ID, as it is completely arbitrary. It's more like a name, than a number).\n",
    "# \n",
    "# The targets are a Boolean variable (so 0, or 1). We are taking a period of 2 years in our inputs, and the next 6 months as targets. So, in fact, we are predicting if: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. 6 months sounds like a reasonable time. If they don't convert after 6 months, chances are they've gone to a competitor or didn't like the Audiobook way of digesting information. \n",
    "# \n",
    "# The task : create a machine learning algorithm, which is able to predict if a customer will buy again. \n",
    "# \n",
    "# This is a classification problem with two classes: won't buy and will buy, represented by 0s and 1s. \n",
    "\n",
    "# %% [markdown]\n",
    "# ### Importing the relevant libraries\n",
    "\n",
    "# %%\n",
    "# import numpy as np  #Already imported\n",
    "import tensorflow as tf\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Data\n",
    "\n",
    "# %%\n",
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "train_inputs = npz['inputs'].astype(np.float64)\n",
    "train_targets = npz['targets'].astype(np.int32)\n",
    "\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "\n",
    "validation_inputs = npz['inputs'].astype(np.float64)\n",
    "validation_targets = npz['targets'].astype(np.int32)\n",
    "\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "\n",
    "test_inputs = npz['inputs'].astype(np.float64)\n",
    "test_targets = npz['targets'].astype(np.int32)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Model\n",
    "\n",
    "# %% [markdown]\n",
    "# Outline, Optimizers, Loss, Early Stopping and Training\n",
    "\n",
    "# %%\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50\n",
    "\n",
    "# Define the model using the Input layer explicitly\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),  # First hidden layer with ReLU activation function.\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),  # Second hidden layer with ReLU activation function.\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')  # Output layer with softmax activation function for probability distribution.\n",
    "])\n",
    "\n",
    "# Compile the model with the specified configurations\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "max_epochs = 100\n",
    "\n",
    "model.fit(train_inputs, train_targets, batch_size=batch_size, epochs=max_epochs, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
    "\n",
    "# %%\n",
    "# Here’s a summary of what to look for in your results:\n",
    "\n",
    "# Training vs. Validation Accuracy: If training accuracy continues to improve while validation accuracy plateaus or decreases, this is a sign of overfitting.\n",
    "# Training vs. Validation Loss: If training loss continues to decrease while validation loss plateaus or increases, this is another sign of overfitting.\n",
    "\n",
    "# Based on your results:\n",
    "\n",
    "# Training Accuracy: Increases steadily, reaching around 83-84% in the final epochs.\n",
    "# Validation Accuracy: Fluctuates but generally remains stable around 83-85%, with some variation.\n",
    "# Training Loss: Decreases consistently over epochs.\n",
    "# Validation Loss: Decreases initially but starts to fluctuate around epoch 40, without significant improvement or degradation.\n",
    "\n",
    "# While there is no drastic increase in validation loss or significant divergence between training and validation accuracy, \n",
    "# the fluctuation and plateauing of validation accuracy and loss suggest the model may be starting to overfit, particularly after epoch 40. \n",
    "# The model learns well initially, but its generalization ability does not improve as much beyond a certain point.\n",
    "\n",
    "# %%\n",
    "input_size = 10  # Number of input features\n",
    "output_size = 2  # Number of output classes\n",
    "hidden_layer_size = 50  # Number of neurons in the hidden layers\n",
    "\n",
    "# Define the model using the Input layer explicitly\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),  # First hidden layer with ReLU activation function.\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),  # Second hidden layer with ReLU activation function.\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')  # Output layer with softmax activation function for probability distribution.\n",
    "])\n",
    "\n",
    "# Compile the model with the specified configurations\n",
    "model.compile(optimizer='adam',  # Optimizer to use\n",
    "              loss='sparse_categorical_crossentropy',  # Loss function for classification\n",
    "              metrics=['accuracy'])  # Metric to evaluate during training\n",
    "\n",
    "batch_size = 100  # Number of samples per gradient update\n",
    "max_epochs = 100  # Maximum number of epochs for training\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)  # Early stopping callback to prevent overfitting\n",
    "\n",
    "# Train the model on the training data with validation\n",
    "model.fit(train_inputs, train_targets,  # Training data and labels\n",
    "          batch_size=batch_size,  # Batch size for training\n",
    "          epochs=max_epochs,  # Maximum number of epochs\n",
    "          callbacks=[early_stopping],  # Callbacks to use during training\n",
    "          validation_data=(validation_inputs, validation_targets),  # Validation data and labels\n",
    "          verbose=2)  # Verbosity mode\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test Model\n",
    "\n",
    "# %%\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "\n",
    "# %%\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy *100))\n",
    "\n",
    "# %%\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = model.predict(test_inputs)\n",
    "test_predicted_classes = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Calculate additional KPIs\n",
    "precision = precision_score(test_targets, test_predicted_classes)\n",
    "recall = recall_score(test_targets, test_predicted_classes)\n",
    "f1 = f1_score(test_targets, test_predicted_classes)\n",
    "conf_matrix = confusion_matrix(test_targets, test_predicted_classes)\n",
    "roc_auc = roc_auc_score(test_targets, test_predictions[:, 1])  # Assuming binary classification\n",
    "\n",
    "# Print the KPIs\n",
    "print('Precision: {:.2f}'.format(precision))\n",
    "print('Recall: {:.2f}'.format(recall))\n",
    "print('F1 Score: {:.2f}'.format(f1))\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print('AUC-ROC: {:.2f}'.format(roc_auc))\n",
    "\n",
    "# %%\n",
    "# Summary\n",
    "# Precision (0.88): 88% of the customers predicted to buy again actually did buy again.\n",
    "# Recall (0.77): 77% of the customers who actually bought again were correctly identified by the model.\n",
    "# F1 Score (0.82): A good balance between precision and recall.\n",
    "# Confusion Matrix: Provides a detailed breakdown of true positives, false positives, true negatives, and false negatives.\n",
    "    # True Negatives (TN): 194 - The number of customers who were correctly identified as not buying again.\n",
    "    # False Positives (FP): 25 - The number of customers who were incorrectly identified as buying again, but actually did not buy again.\n",
    "    # False Negatives (FN): 52 - The number of customers who were incorrectly identified as not buying again, but actually did buy again.\n",
    "    # True Positives (TP): 177 - The number of customers who were correctly identified as buying again.\n",
    "# AUC-ROC (0.92): Excellent ability of the model to distinguish between classes.\n",
    "    # AUC-ROC values range from 0 to 1.\n",
    "    # An AUC-ROC value of 0.5 indicates a model with no discrimination ability, equivalent to random guessing.\n",
    "    # An AUC-ROC value of 1.0 indicates a perfect model.\n",
    "    # In your case, an AUC-ROC of 0.92 means that the model has excellent discrimination ability and is very good at distinguishing between customers who will buy again and those who will not.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
